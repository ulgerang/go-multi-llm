package gemini

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"os"
	"strings"

	"github.com/ulgerang/llm-module/llm"
	"github.com/ulgerang/llm-module/logger"
	"github.com/ulgerang/llm-module/utils"

	"google.golang.org/genai"
)

const defaultGeminiModel = "gemini-pro"

// Provider implements llm.Provider for Google's Gemini models.
type Provider struct {
	client    *genai.Client
	logger    logger.Logger
	modelName string
}

// New creates a new Gemini provider instance.
func New(log logger.Logger, apiKey, modelName string) (*Provider, error) {
	if apiKey == "" {
		apiKey = os.Getenv("GEMINI_API_KEY")
		if apiKey == "" {
			return nil, errors.New("GEMINI_API_KEY not provided")
		}
	}

	if modelName == "" {
		modelName = os.Getenv("GEMINI_MODEL")
		if modelName == "" {
			modelName = defaultGeminiModel
		}
	}

	ctx := context.Background()

	var (
		client *genai.Client
		err    error
	)

	if os.Getenv("GEMINI_USING_VERTEXAI") == "true" {
		client, err = genai.NewClient(ctx, &genai.ClientConfig{
			Project:  os.Getenv("GEMINI_PROJECT"),
			Location: os.Getenv("GEMINI_LOCATION"),
			Backend:  genai.BackendVertexAI,
		})
	} else {
		client, err = genai.NewClient(ctx, &genai.ClientConfig{APIKey: apiKey})
	}

	if err != nil {
		return nil, fmt.Errorf("failed to create Gemini client: %w", err)
	}

	return &Provider{client: client, logger: log, modelName: modelName}, nil
}

// GetModelName returns the configured Gemini model name.
func (p *Provider) GetModelName() string {
	return p.modelName
}

// GenerateText performs a non-streaming Gemini request.
func (p *Provider) GenerateText(ctx context.Context, prompt string, opts ...llm.GenerationOption) (string, *llm.UsageInfo, error) {
	options := &llm.GenerationOptions{
		Temperature: llm.ValuePtr(float32(0.7)),
		MaxTokens:   llm.ValuePtr(int32(4096)),
		TopK:        llm.ValuePtr(float32(40)),
		TopP:        llm.ValuePtr(float32(0.95)),
		System:      "You are a helpful assistant.",
	}
	for _, opt := range opts {
		opt(options)
	}

	maxTokens := int32(4096)
	if options.MaxTokens != nil {
		maxTokens = *options.MaxTokens
	}

	config := &genai.GenerateContentConfig{
		Temperature:     options.Temperature,
		TopK:            options.TopK,
		TopP:            options.TopP,
		MaxOutputTokens: maxTokens,
	}

	// Build combined system instruction.
	var systemBuilder strings.Builder
	if len(options.SystemBlocks) > 0 {
		for _, block := range options.SystemBlocks {
			systemBuilder.WriteString(block.Text)
			systemBuilder.WriteString("\n\n")
		}
	}
	if options.System != "" {
		systemBuilder.WriteString(options.System)
		systemBuilder.WriteString("\n\n")
	}
	if options.Language != "" && options.Language != "en" {
		systemBuilder.WriteString(fmt.Sprintf("Please respond in %s language.", utils.GetLangName(options.Language)))
	}

	finalSystemInstruction := strings.TrimSpace(systemBuilder.String())
	if finalSystemInstruction != "" {
		config.SystemInstruction = &genai.Content{
			Role:  genai.RoleModel,
			Parts: []*genai.Part{{Text: finalSystemInstruction}},
		}
	}

	contents := []*genai.Content{{
		Role:  genai.RoleUser,
		Parts: []*genai.Part{{Text: prompt}},
	}}

	if options.ResponseFormat != "" {
		config.ResponseMIMEType = options.ResponseFormat
	}

	if options.ResponseSchema != nil {
		config.ResponseMIMEType = "application/json"
		schema, err := schemaToGenaiSchema(options.ResponseSchema)
		if err != nil {
			return "", nil, err
		}
		config.ResponseSchema = schema
	}

	if options.AllowSexualContent {
		config.SafetySettings = []*genai.SafetySetting{
			{Category: genai.HarmCategorySexuallyExplicit, Threshold: genai.HarmBlockThresholdOff},
			{Category: genai.HarmCategoryCivicIntegrity, Threshold: genai.HarmBlockThresholdOff},
			{Category: genai.HarmCategoryHateSpeech, Threshold: genai.HarmBlockThresholdOff},
			{Category: genai.HarmCategoryDangerousContent, Threshold: genai.HarmBlockThresholdOff},
		}
	}

	if strings.Contains(p.modelName, "2.5-flash") {
		zero := int32(0)
		config.ThinkingConfig = &genai.ThinkingConfig{IncludeThoughts: false, ThinkingBudget: &zero}
	}

	resp, err := p.client.Models.GenerateContent(ctx, p.modelName, contents, config)
	if err != nil {
		p.logger.Error(fmt.Sprintf("Failed to generate Gemini content: %v", err), err)
		return "", nil, err
	}

	if resp == nil || len(resp.Candidates) == 0 || resp.Candidates[0].Content == nil {
		p.logger.Warning("No content generated by Gemini")
		return "", nil, errors.New("no content generated")
	}

	var generated strings.Builder
	for _, part := range resp.Candidates[0].Content.Parts {
		generated.WriteString(part.Text)
	}

	text := generated.String()
	if text == "" {
		return "", nil, errors.New("unexpected empty Gemini response")
	}

	usage := convertGeminiUsage(resp)
	p.logger.Info(fmt.Sprintf("Generated text (Gemini): %s", text))
	return text, usage, nil
}

// GenerateTextStream streams responses from Gemini.
func (p *Provider) GenerateTextStream(ctx context.Context, prompt string, outChan chan<- llm.StreamChunk, opts ...llm.GenerationOption) (*llm.UsageInfo, error) {
	defer close(outChan)

	options := &llm.GenerationOptions{
		Temperature: llm.ValuePtr(float32(0.7)),
		MaxTokens:   llm.ValuePtr(int32(4096)),
		TopK:        llm.ValuePtr(float32(40)),
		TopP:        llm.ValuePtr(float32(0.95)),
		System:      "You are a helpful assistant.",
	}
	for _, opt := range opts {
		opt(options)
	}

	maxTokens := int32(4096)
	if options.MaxTokens != nil {
		maxTokens = *options.MaxTokens
	}

	config := &genai.GenerateContentConfig{
		Temperature:     options.Temperature,
		TopK:            options.TopK,
		TopP:            options.TopP,
		MaxOutputTokens: maxTokens,
	}

	if len(options.SystemBlocks) > 0 {
		options.System = options.SystemBlocks[0].Text
	}

	var systemBuilder strings.Builder
	if len(options.SystemBlocks) > 0 {
		for _, block := range options.SystemBlocks {
			systemBuilder.WriteString(block.Text)
			systemBuilder.WriteString("\n\n")
		}
	}
	if options.System != "" {
		systemBuilder.WriteString(options.System)
		systemBuilder.WriteString("\n\n")
	}
	if options.Language != "" && options.Language != "en" {
		systemBuilder.WriteString(fmt.Sprintf("Please respond in %s language.", utils.GetLangName(options.Language)))
	}

	finalSystemInstruction := strings.TrimSpace(systemBuilder.String())
	if finalSystemInstruction != "" {
		config.SystemInstruction = &genai.Content{
			Role:  genai.RoleModel,
			Parts: []*genai.Part{{Text: finalSystemInstruction}},
		}
	}

	contents := []*genai.Content{{
		Role:  genai.RoleUser,
		Parts: []*genai.Part{{Text: prompt}},
	}}

	if options.ResponseFormat != "" {
		config.ResponseMIMEType = options.ResponseFormat
	}
	if options.ResponseSchema != nil {
		p.logger.Warning("ResponseSchema requested with Gemini streaming; client-side parsing will be required.")
		config.ResponseMIMEType = "application/json"
	}

	if options.AllowSexualContent {
		config.SafetySettings = []*genai.SafetySetting{
			{Category: genai.HarmCategoryHarassment, Threshold: genai.HarmBlockThresholdBlockNone},
			{Category: genai.HarmCategoryHateSpeech, Threshold: genai.HarmBlockThresholdBlockNone},
			{Category: genai.HarmCategorySexuallyExplicit, Threshold: genai.HarmBlockThresholdBlockNone},
			{Category: genai.HarmCategoryDangerousContent, Threshold: genai.HarmBlockThresholdBlockNone},
		}
	}

	p.logger.Info("Starting Gemini streaming generation")

	iter := p.client.Models.GenerateContentStream(ctx, p.modelName, contents, config)

	var finalResp *genai.GenerateContentResponse

	for resp, err := range iter {
		if err != nil {
			p.logger.Error(fmt.Sprintf("Error reading Gemini stream: %v", err), err)
			outChan <- llm.StreamChunk{Err: fmt.Errorf("stream read error: %w", err)}
			return &llm.UsageInfo{}, err
		}

		if resp == nil {
			continue
		}

		finalResp = resp

		var deltaBuilder strings.Builder
		if len(resp.Candidates) > 0 && resp.Candidates[0].Content != nil {
			for _, part := range resp.Candidates[0].Content.Parts {
				deltaBuilder.WriteString(part.Text)
			}
		}

		delta := deltaBuilder.String()
		if delta != "" {
			outChan <- llm.StreamChunk{Delta: delta}
		}
	}

	usage := convertGeminiUsage(finalResp)
	if usage.InputTokens == 0 && usage.OutputTokens == 0 {
		p.logger.Warning("Gemini stream finished without usage metadata")
	} else {
		p.logger.Info(fmt.Sprintf("Gemini stream usage: input=%d output=%d", usage.InputTokens, usage.OutputTokens))
	}

	return usage, nil
}

// Close releases the Gemini client resources.
func (p *Provider) Close() error {
	p.logger.Info("[Gemini] Provider closed.")
	return nil
}

func convertGeminiUsage(resp *genai.GenerateContentResponse) *llm.UsageInfo {
	usage := &llm.UsageInfo{}
	if resp != nil && resp.UsageMetadata != nil {
		usage.InputTokens = int(resp.UsageMetadata.PromptTokenCount)
		usage.OutputTokens = int(resp.UsageMetadata.CandidatesTokenCount)
	}
	return usage
}

func schemaToGenaiSchema(property *llm.SchemaProperty) (*genai.Schema, error) {
	if property == nil {
		return nil, errors.New("input SchemaProperty cannot be nil")
	}

	schemaJSON, err := llm.ConvertToJSONSchema(property)
	if err != nil {
		return nil, err
	}

	schema := &genai.Schema{}
	if err := json.Unmarshal([]byte(schemaJSON), schema); err != nil {
		return nil, err
	}

	return schema, nil
}
